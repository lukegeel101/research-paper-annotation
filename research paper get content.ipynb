{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc503c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import unstructured\n",
    "\n",
    "#DETECT FILE TYPE\n",
    "from filetype import guess\n",
    "\n",
    "def detect_document_type(document_path):\n",
    "    \n",
    "    guess_file = guess(document_path)\n",
    "    file_type = \"\"\n",
    "    image_types = ['jpg', 'jpeg', 'png', 'gif']\n",
    "    \n",
    "    if(guess_file.extension.lower() == \"pdf\"):\n",
    "        file_type = \"pdf\"\n",
    "        \n",
    "    elif(guess_file.extension.lower() in image_types):\n",
    "        file_type = \"image\"\n",
    "            \n",
    "    else:\n",
    "        file_type = \"unkown\"\n",
    "            \n",
    "    return file_type\n",
    "\n",
    "# EXTRACT DOCUMENTS CONTENT\n",
    "\n",
    "from langchain.document_loaders.image import UnstructuredImageLoader\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "def extract_file_content(file_path):\n",
    "    \n",
    "    file_type = detect_document_type(file_path)\n",
    "    \n",
    "    if(file_type == \"pdf\"):\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "        \n",
    "    elif(file_type == \"image\"):\n",
    "        loader = UnstructuredImageLoader(file_path)\n",
    "        \n",
    "    document = loader.load()\n",
    "    documents_content = '\\n'.join(doc.page_content for doc in document)\n",
    "    \n",
    "    return documents_content\n",
    "\n",
    "#CREATE CHUNKS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len)\n",
    "\n",
    "#CREATEE EMBEDDINGS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#CREATE VECTOR INDEX\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def get_doc_search(text_splitter):\n",
    "    \n",
    "    return FAISS.from_texts(text_splitter, embeddings)\n",
    "\n",
    "#START CHATTING WITH YOUR DOCUMENT\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type = \"map_rerank\",\n",
    "                     return_intermediate_steps = True)\n",
    "\n",
    "def chat_with_files(file_path, query):\n",
    "    \n",
    "    file_content = extract_file_content(file_path)\n",
    "    file_splitter = text_splitter.split_text(file_content)\n",
    "    \n",
    "    document_search = get_doc_search(file_splitter)\n",
    "    documents = document_search.similarity_search(query)\n",
    "    \n",
    "    results = chain({\n",
    "                        \"input_documents\":documents,\n",
    "                        \"question\": query\n",
    "                    },\n",
    "                    return_only_outputs = True)\n",
    "    results = results['intermediate_steps'][0]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85727f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1160, which is longer than the specified 1000\n",
      "Created a chunk of size 1154, which is longer than the specified 1000\n",
      "Created a chunk of size 1476, which is longer than the specified 1000\n",
      "Created a chunk of size 1017, which is longer than the specified 1000\n",
      "Created a chunk of size 1636, which is longer than the specified 1000\n",
      "Created a chunk of size 1109, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Orthography to Semantics: a Study of Morphological Processing through Deep Learning Neural Networks\n",
      "\n",
      "Jaime D´avila School of Cognitive Science Hampshire College Amherst, MA, USA Email: jdavila@hampshire.edu\n",
      "\n",
      "Joanna Morris School of Cognitive Science Hampshire College Amherst, MA, USA Email: jmorris@hampshire.edu\n",
      "\n",
      "Abstract—This paper presents results of using deep learning neural network techniques to map words, represented at the levels of letters, into semantic, distributed vector representations. We study and compare algorithms that have been proposed as models for human orthographic and morphological processing, such as the two layers symbolic network (AKA Naive Discrimi- natory Learning network) proposed by Baayen et al [1], and the dual-route approach presented by Grainger and Ziegler [2]. In addition, we study the effect of representing letters as one-hot vectors or via distributed vector representations, much like the natural language processing ﬁeld has done for words. Experiment results show a better performance for the dual-route algorithm using distributed letter representations, both in terms of accuracy and speed. Our results are obtained with training sets in the tens of thousands of words, as opposed to the hundreds of thousands of words used elsewhere in the literature. These results point to the feasibility of taking advantage of morpho-orthographic patterns language in order to assist with word processing in natural processing tasks.\n",
      "\n",
      "I. INTRODUCTION\n",
      "\n",
      "Research in neural network topologies and applications has demonstrated the ability of these computation devices to detect and efﬁciently use patterns in natural language processing tasks. More recently, these abilities have been advanced by making use of techniques such as convolutions in conjunction with massive amounts of data. For example, Krizhevsky, Sutskever, and Hinton obtained state-of-the-art results in clas- sifying images from the ImageNet LSVRC-2010 contest [3], and Mikolov et al developed a model that mapped one-hot- vector representations of words into a distributed vector space with interesting syntactic and semantic properties [4]. One characteristic of many of the algorithms used to generate such results is the fact that they need both a big training set and long training times [7].\n",
      "\n",
      "In this paper we present results of training neural networks with techniques such as convolution, signal dimentionality reduction, cross-entropy, and others, in order to test com- putational algorithms proposed by other researchers in the linguistics literature as ways to map written letters into word semantic representations. In particular, we concentrate on the way in which input representations and details in topology\n",
      "\n",
      "affect network performance, both from the point of view of ac- curacy and speed, since past research has found convolutional neural networks to be sensitive to such design decisions ([5], [7]). Our empirical results demonstrate considerable speed differences between networks using different topologies and input formats on the task of recreating the original word labels after passing them through a process that generates distributed vector representations. This result, specially in light of the smaller training set we use, could point to advantages of using input representations at the level of letters in natural language processing tasks, as they could facilitate taking advantage of morphological information. By doing so, neural networks could generalize into words they have not been pre- viously exposed to by developing word representations from the combination of word roots and additional morphemes. For example, with the ability to code the fact that verb + er generates a noun for someone that performs that verb action, a network could more easily learn a good semantic representation for hunter or runner after only having seen walker.\n",
      "\n",
      "The rest of this paper is divided as follows: section II presents two models previously proposed by other researchers in the linguistics literature as mechanisms for mapping or- thographic symbols into semantic space, one which uses morphological information and one that does not; section III quickly discusses previous neural network architectures and other techniques used to map words into a distributed vector space; section IV presents the deep learning architecture used in our experiments; section V describes the conﬁguration parameters for our experiments; section VI presents empirical results we have obtained in our experiments; section VII discusses our results and presents conclusions; and section VIII suggest possible areas for future research based on the results we have obtained.\n",
      "\n",
      "II. ORTHOGRAPHY PROCESSING MODELS\n",
      "\n",
      "A. Dual-route Approach\n",
      "\n",
      "Within the ﬁeld of linguistics, Jonathan Grainger and Jo- hannes Ziegler have proposed that human processing of words\n",
      "\n",
      "involves mapping from orthographic elements into semantics via two separate routes [2]. One of these routes selects com- binations of letters that are the most informative in selecting word identity, called diagnosticity constraints. A second path in this model involves detecting sub-lexical phonological and morphological representations, called chunking constraints. These two paths lead to a combination of ﬁne-grained and coarse grained processing, which together allow for fast pro- cessing of orthographic symbols. A sketch of this model is shown in ﬁgure 1.\n",
      "\n",
      "represented via a set of N input nodes. With this format, each word is presented to a neural network by activating one of these N nodes and leaving all other nodes inactive. In this manner, we can constructs a training set from online corpora by reading the text, dividing it into words, and automatically turning it into a sequence of one-hot vectors. This type of representation, therefore, allows us to leverage huge amounts of available data without having to tag each input element manually. At the same time, this type of representation keeps us from taking advantage of commonalities between different inputs that nevertheless share characteristics. For example, because the word runs and the word walks would be rep- resented by orthogonal vectors, a machine learning algorithm processing written text would have no knowledge that these are both verbs of movement/displacement in present tense singular form.\n",
      "\n",
      "Fig. 1. The Dual-route Orthographic Processing Model proposed by Grainger and Ziegler\n",
      "\n",
      "to our research because it argues that an intermediate step in word processing is based on morpheme processing.\n",
      "\n",
      "This topology is particularly important\n",
      "\n",
      "Much work, therefore, has gone into how to turn one-hot vector representations into distributed vector representations. Mikolov et al [4] have presented a process through which one- hot-vector representations of words are turned into distributed vector representations by training a network to predict the context in which words appear in sentences, known as the skip- gram model. This approach is sketched in ﬁgure 2. Mikolov also discusses elsewhere in the literature how different al- gorithms for generating these vectors, or word embeddings, produce different results in syntactic and semantic tasks. In their experiments, these learning algorithms made use of between hundreds of thousands and billions of words in their training sets [7].\n",
      "\n",
      "B. Two-layer Symbolic Network Model\n",
      "\n",
      "Baayen, Milin, Durdevic, Hendrix, and Merelli have ex- perimented with networks that map words into distributed representations coding the probability distribution of inﬂected forms without intervening morphological representations in highly agglutinating languages like Serbian [1]. These net- works, which connect their input layers directly to their output layers (i.e. do not make use of hidden layers) were found to be successful at predicting the kind of effects we see in humans, documented elsewhere in the morpheme processing literature. In their network model, word representations at the input layer were directly connected to the network’s output layer. Of particular importance to the experiments discussed in our paper is the fact this model, also known as the Naive Discriminatory Learning model, argues that word processing can be done without intermediate morphological representations.\n",
      "\n",
      "that\n",
      "\n",
      "III. WORD2VEC MAPPING OF WORDS FROM ONE-HOT-VECTORS TO DISTRIBUTED VECTOR REPRESENTATIONS\n",
      "\n",
      "In a one-hot-vector representation of words, each word is represented via a set of nodes with cardinality equal to the number of words in the vocabulary. That is, N words are\n",
      "\n",
      "Fig. 2. Skip-gram model. Provided with word w(t), learn to predict its context words.\n",
      "\n",
      "When training networks to correctly predict the probability of context words, Mikolov et al found that word projections into distributed vector space encoded linguistic patterns and regularities. For example, taking the vector for ”king”, sub- tracting the vector for ”man”, and then adding the vector for ”woman” produced a vector that was closest to the vector for ”queen” than to any other word.\n",
      "\n",
      "IV. FROM ONE-HOT-VECTOR REPRESENTATIONS OF LETTERS TO DISTRIBUTED SEMANTIC REPRESENTATIONS OF WORDS\n",
      "\n",
      "In order to study the two different morpho-phonetic theories presented by Grainger and Ziegler and by Baayen et al, we have implemented neural network topologies that follow the concepts they describe, and tested them on online corpora. The experiments discussed in this paper implemented neural networks that incorporated concepts from these theories using several deep learning techniques. These networks received words as a sequence of one-hot-vector representation of letters and mapped them to word vector space through convolutions, noise contrastive estimation (NCE) [6] [4], dimentionality reduction, and cross-entropy learning. Having networks re- ceive letters as one-hot vectors also allowed us to incorporate mapping from orthography to representations at the level of words, something that both models mentioned above do. Like in the case of representing written text as a sequence of words in one-hot vector format, using one-hot vector representations of letters allows us to take advantage of online corpora, while also keeping us from taking advantage of commonalities be- tween letters. For example, no information would be provided informing an algorithm that e and a are both vowels. Mapping from one-hot vector representations of letters to a distributed representation of letters would allow us to take advantage of these commonalities, while at the same time allowing us to study morpho-orthographic linguistic theories of semantic word representations.\n",
      "\n",
      "Following is a description of these networks in detail, before\n",
      "\n",
      "presenting results.\n",
      "\n",
      "A. From One-hot-vector Representation of Letters to a Dis- tributed Vector Space for Letters\n",
      "\n",
      "Fig. 3. Letters are mapped from one-hot-vectors to distributed vectors by a network that learns to predict the context in which letters are used.\n",
      "\n",
      "letters in one-hot-vector format. These network inputs had a cardinality of 784, equal to the length of the longest word (28) multiplied by the size of the letters one-hot vector represen- tations (28). These letters were then mapped to a continuous vector space by the letter embedding matrix developed above, thus generating the representation of a word as a stream of letters in continuous vector space. As the longest word in the vocabulary was 28 letters long, words at this point were represented by a stream of 28 X 5 = 140 ﬂoating point numbers. Words shorter than 28 letters long were zero-padded to the left and right. This part of the process is represented in ﬁgure 4 .\n",
      "\n",
      "In our networks, letters were mapped to a vector space of cardinality 5. We chose this cardinality because we can represent 28 distinct letters with 5 nodes even if we used a purely binary representation. A purely binary representation could have led to letters being represented in ways where different letters did not share values for common attributes, but as we we will discuss later in this paper, this is not what the learning process converged towards. Learned letter representations both grouped similar types of letters together and were useful for the learning process in the rest of the network. This process generated a 28 X 5 letter embedding matrix to be used in other stages of processing, and is represented in ﬁgure 3.\n",
      "\n",
      "Once this mapping to distributed space is completed, we use the resulting mapping matrix in a convolution that turns words, represented as a sequence of letters in one-hot-vector format, to words represented as the concatenation of distributed vec- tors for those same letters, as discussed further in the following sections.\n",
      "\n",
      "Fig. 4. Words were mapped from a sequence of one-hot-vector letter representations to a sequence of distributed vector letter representation.\n",
      "\n",
      "B. Dual Route Convolutions\n",
      "\n",
      "After the training described above, we had a network that received a single word at a time, represented as a sequence of\n",
      "\n",
      "In order to simulate Grainger and Ziegler’s dual route pro- cessing architecture, this representation was then fed through two parallel convolution layers. One of them, simulating the\n",
      "\n",
      "chunking process of detecting consecutive letters, had the width of two letter representations (ten nodes, in our case), with a stride of one letter representation at a time, or ﬁve nodes. The second convolution layer, simulating the diagnos- ticity process of detecting letters in a word, was as wide as the longest word representation in the language, or 140 nodes. This convolution layer did not have a stride, as it covered the complete word at once. Both of these convolution layers had a depth of 784 (28 letters, squared), allowing the network to use each ﬁlter to detect a speciﬁc letter pair, if needed. This produced an output from the diagnosticity convolution of 21168 nodes (784 possible letter pairs, and a stride of one letter 27 times) and an output from the chunking convolution of 784 nodes (784 possible letter pairs, with no stride).\n",
      "\n",
      "C. Generation of Word Semantics and Word Labels\n",
      "\n",
      "All of these values where then passed to a fully connected layer with 50 nodes, reducing the data dimentionality. The value of 50 nodes was tested empirically, and produced better results than networks with 75, 100, 200, or 30 nodes at that particular layer. Values were then fed to a layer with 128 nodes, trained to output the distributed word representation obtained in the original word embedding process. For both of these distributed word representations, 128 nodes worked better than both 75 nodes and 256 nodes. Finally, in order to ensure that training moved parameters towards values that allowed recovering the original word, these 128 nodes were fed to an output layer with 50,000 nodes, which was trained via negative log likelihood to output the label for the word entered. The resulting complete network is shown in ﬁgure 5.\n",
      "\n",
      "which they propose implements a Naive Discriminatory Learn- ing algorithm. Although they did not use their topology for the exact task we use here, they did train networks receiving information about co-occurrences of linguistic elements in corpora, which makes it an interesting topology to compare ours with, specially since their networks achieved results that mimicked human word processing without making use of ex- plicit morphological representations. Our version of a network that follows their approach starts by presenting words as one hot vector sequences of letters, convolving them with a letter embedding matrix constructed as before, and then directly feeding it to a layer that is trained to generate distributed word representations. This topology is shown in ﬁgure 6. As can be seen, the input and output signals to these two networks are the same, and the only difference is that the symbolic two layer network does not use dual route convolutions or dimension reduction layers.\n",
      "\n",
      "Fig. 6. Naive Discriminatory Learning network\n",
      "\n",
      "In order to test the effect of letter representations for these two topologies, we also ran experiments where the one hot vector representations of letters where not fed through the letter embedding matrix, instead feeding them directly to the rest of the network, with the appropriate changes in the dual route convolutions for those networks, and with no additional changes for the symbolic two layer networks. Finally, we tested the dual route networks with either the discriminality convolution by itself or the chunking convolution by itself in order to see what effect they each have on total network performance.\n",
      "\n",
      "V. EXPERIMENT PARAMETERS\n",
      "\n",
      "Fig. 5. Complete Dual-Route NN architecture\n",
      "\n",
      "D. Comparison Networks\n",
      "\n",
      "In order to better evaluate the performance of the network described above, we ran the same task under other topologies. One of these topologies mimicked the symbolic two later net- work ﬁrst presented by Baayen, Milin, Hendrix, and Merelli,\n",
      "\n",
      "sentences trained All at previously from Wikipedia, http://www.mattmahoney.net/dc/textdata.html. exact vocabulary was formed by the 50,000 most common words in this set, replacing all other words with the token UNKNOWN. To generate word embeddings, the skip-gram model was conﬁgured to use two context words to the left and two context words to the right of the input word. We used the\n",
      "\n",
      "networks\n",
      "\n",
      "obtained\n",
      "\n",
      "were\n",
      "\n",
      "with\n",
      "\n",
      "available The\n",
      "\n",
      "same data set this time with to train letter embeddings, one context letter to the left and to the right. Both types of embeddings were trained using gradient descent with a learning rate of 1. Convolution, dimentionality reduction, and softmax layers in the implementations of the dual route and the two-layer layers used a learning rate of .01. Initially, letter word embeddings were trained for 200,000 epochs, embeddings were trained for 100,000 epochs, and dual route convolution layers were trained for 100,000 epochs. Each of these epochs used a different subset of data for training. initially training embedding matrices In all cases, after separately from complete networks, their values continued to be modiﬁed while they were part of other more complex networks. All networks were implemented in the Python programming language through the TensorFlow library [8]. All experiments were repeated 32 times, and the results presented in the next section discuss the average of these 32 runs.\n",
      "\n",
      "VI. RESULTS\n",
      "\n",
      "A. Letter Embeddings\n",
      "\n",
      "After networks were trained to predict the context in which letters were being used in the vocabulary, we looked at the embedding matrices that emerged, ﬁnding some patterns to the coding assigned to particular letters. Considering these embeddings as vectors in representation space, and computing the angle for vectors representing different pairs of letters, smaller angles indicated letters being close to each other in the vector space developed by the neural network learning process. This analysis revealed some expected commonalities based on the ways letters appear in words. For example, vector representations for vowels were most similar to that of other vowels, and ’i’ and ’y’ were always the closest letter to each other. Many other proximities were more obscure, specially among consonants, and will require further study to see what morpho-orthographic information they reveal. The consistent results seen on vowels, though, indicate the learning process can in fact capture letter use commonalities.\n",
      "\n",
      "B. Effect of Topologies and Letter Representations on Learn- ing\n",
      "\n",
      "achieved much lower performance, regardless of number of epochs or learning rates used; those experiments were run for up to a million epochs, and with learning rates of .01, .1, .5, 1, and 5, never achieving an accuracy higher than .125.\n",
      "\n",
      "While the ﬁnal accuracy values for these two topologies are very close to each other, the situation is different when we look at the negative log likelihood error values produced during the learning process. Because we trained networks to produce a single discrete label for each word, the error at the output layer used during gradient descent is given by e = − (cid:80)n 1 log(qi), where n is the size of the training set, and qi is the output for the node in the softmax output layer that corresponds to the correct label for each word in the training set. As seen in the error curves in ﬁgures 7 and 8, networks using letter embedding reach relatively stable values of training errors after about 20,000 epoch, while it takes much longer for networks that do not make use of distributed letter representations. After 20,000 epochs, dual route networks that used distributed letter representations had training error values of between 5.9 and 6.8, while networks that did not use distributed letter representations occasionally produced errors as high as 11.2. This difference in sensitivity to training data begins after networks continue to be trained once they have initially reached stable values, tending to indicate networks not using distributed letter representations are beginning to overﬁt for particular data characteristics in their training data. The two curves shown here display negative log error values as opposed to accuracy values, since it is the former that is used to drive gradient descent in these networks, and it is computationally expensive to determine the accuracy of a network after each training epoch.\n",
      "\n",
      "TABLE I FINAL ACCURACY VALUES\n",
      "\n",
      "Topology Dual Route, Letter Embeddings Dual Route, One-hot Vectors Naive Discriminatory Learning, Letter Embeddings Naive Discriminatory Learning, One-hot Vectors Chunking Path Only Discriminatory Path Only\n",
      "\n",
      "Accuracy .4309 .4173 .4004 .1379 .4197 .3469\n",
      "\n",
      "1) Dual-route topologies: Different network topologies performed differently depending on their using an interme- diate mapping to distributed letter representations or not. For networks with dual path topologies, the average accuracy when using letter embeddings and when not using them was of .4309 and .4173, respectively. While accuracy values of .4309 and .4173 might seem low, being below 5o% accuracy, we note that these networks are being required to assign a correct word label from among 50,000 possibilities. As a comparison, Mikolov et al report performance values of between 9% and 64% in a number of syntactic and semantic tasks, all while using between 24 million and 784 million words ([4]), as opposed to the 50,000 we use in our experiments. In addition, directly recreating word labels after only generating word embeddings, without processing words at the level of letters,\n",
      "\n",
      "Fig. 7. Training of dual route networks using letter embeddings\n",
      "\n",
      "2) Symbolic Two layer (Naive Discriminatory Learning) topologies: Symbolic two layer (Naive Discriminatory Learn- ing) topologies perform slightly worse than dual route net-\n",
      "\n",
      "exact, particular example yet. In addition, networks provided with topological details that allow them to consider pairs of consecutive letters tend to perform better, suggesting that mor- phological information might be being captured and used. In addition, Naive Discriminatory Learning networks, which have no explicit mechanisms for processing/storing morphological information, take longer to achieve optimum accuracy, and are, even at that point, lower than those achieved by dual route topologies.\n",
      "\n",
      "Fig. 8. Training of dual route networks using one-hot vector letter represen- tations\n",
      "\n",
      "VIII. FUTURE WORK\n",
      "\n",
      "works, with an average accuracy of .4004. These networks are much more sensitive to input format, whoever, having an accuracy of only .1379 when using one-hot vector letter representations. Another important difference between dual route and Naive Discriminatory Learning networks is the time that they take to achieve their ﬁnal accuracy values. As seen in ﬁgure 9, even under the best performing input representation, Naive Discriminatory Learning topologies take close to ten times as many epochs to achieve negative log likelihood values similar to those of dual route topologies.\n",
      "\n",
      "The results presented here bring to mind several interesting avenues for future investigation. We plan to look at the nature of errors being made by networks of different topologies in order to better understand how they are obtaining their results, and what facilitates learning speed and resilience for the dual path letter embedding topologies. We are also interested in studying the letter proximities calculated from the letter embedding matrices, as it might be these proximities that help the learning process under a comparatively smaller training set. We also plan on controlling the number of letter combinations that are checked for in the letter convolution layers, as many letter combinations are bound to not be present in our vocabulary, and knowing which letter combinations are most important during the learning process could provide insights into language processing of both artiﬁcial neural network and humans. Finally, our future research will look into chunking paths that consider longer letter sequences, allowing for longer morphological units to be considered, such as -ing or -ment.\n",
      "\n",
      "ACKNOWLEDGMENT\n",
      "\n",
      "Fig. 9. Training of Naive Discriminatory Learning topology using letter embedding representations\n",
      "\n",
      "C. Effect of Separate Routes on Dual Route Topologies\n",
      "\n",
      "Neither route of the two route topology, taken separately, was able to achieve accuracy values as high as those obtained by the complete dual route topology, but chunking came close to it. Chunking paths achieved an average accuracy of .4197, while Diagnosticity paths achieved an average accuracy of .3469. This seems to indicate that most of the work of the network is being done by the chunking path, where letter pairs capable of forming morphological units are detected.\n",
      "\n",
      "VII. CONCLUSION\n",
      "\n",
      "The results outlined above indicate that topologies trained with letter embeddings reach optimum accuracy values faster, and are less sensitive to training data selection details. A possible explanation for those speed improvements and re- silience can be the fact that networks do not need to see as many individual examples of letters being used in particu- lar words, since embeddings provide distributed information. This information can be used when processing different but similar letter combinations that might not have been in an\n",
      "\n",
      "The authors would like to thank: Josiah Erickson, Associate Director of IT for Infrastructure and Communication Systems, for his work on installing, conﬁguring, and maintaining the computer system where experiments were run; and Matthew Fullmer, who completed his undergraduate degree at Hamp- shire College in May 2017, for his work with Python code that was later incorporated into the one we used in our experiments.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "[1] Baayen, R. Harald and Milin, Peter and Filipovic Durdevic, Dusica, and Hendrix, Peter, and Marelli, Marco. An amorphous model for morpho- logical processing in visual comprehension based on naive discriminative learning, Psychological Review, 2011.\n",
      "\n",
      "[2] Grainger, Jonathan and Ziegler, Johannes, A Dual-Route Approach to Orthographic Processing, Frontiers in Psychology, Volume 2, 2011 [3] Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. ImageNet Clas- siﬁcation with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 2012\n",
      "\n",
      "[4] Mikolov, Thomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed Representations of Words and Phrases and their Compositionality, Advances in neural Information processing Systems 26 (NIPS 2013).\n",
      "\n",
      "[5] Zhang, Ye, Wallace, Byron. A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classiﬁcation. Proceedings of the 8th International Joint Conference on Natural Lan- guage Processing. 2017.\n",
      "\n",
      "[6] Mnih, Andriy, Kavukcuoglu, Koray. Learning word embeddings efﬁciently with noise-contrastive estimation. Neural Information processing Systems (NIPS) 2013.\n",
      "\n",
      "[7] Mikolov, Thomas, Chen, Kai, Corrado, Greg, Dean, Jeffrey. Efﬁcient Esti- mation of Word Representations in Vector Space International Conference on Learning Representation. 2013\n",
      "\n",
      "[8] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irv- ing, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Man- junath Kudlur, Josh Levenberg, Dan Man, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.\n"
     ]
    }
   ],
   "source": [
    "research_paper1_path = \"/Users/lukegeel/Desktop/research/Jaime/Davila_Morris_WCCI_2018.pdf\"\n",
    "\n",
    "research_paper1_content = extract_file_content(research_paper1_path)\n",
    "research_paper1_chunks = text_splitter.split_text(research_paper1_content)\n",
    "doc_search_paper1 = get_doc_search(research_paper1_chunks)\n",
    "\n",
    "print(research_paper1_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34eaf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From Orthography to Semantics: a Study of Morphological Processing through Deep Learning Neural Networks\\n\\nJaime D´avila School of Cognitive Science Hampshire College Amherst, MA, USA Email: jdavila@hampshire.edu\\n\\nJoanna Morris School of Cognitive Science Hampshire College Amherst, MA, USA Email: jmorris@hampshire.edu', 'Abstract—This paper presents results of using deep learning neural network techniques to map words, represented at the levels of letters, into semantic, distributed vector representations. We study and compare algorithms that have been proposed as models for human orthographic and morphological processing, such as the two layers symbolic network (AKA Naive Discrimi- natory Learning network) proposed by Baayen et al [1], and the dual-route approach presented by Grainger and Ziegler [2]. In addition, we study the effect of representing letters as one-hot vectors or via distributed vector representations, much like the natural language processing ﬁeld has done for words. Experiment results show a better performance for the dual-route algorithm using distributed letter representations, both in terms of accuracy and speed. Our results are obtained with training sets in the tens of thousands of words, as opposed to the hundreds of thousands of words used elsewhere in the literature. These results point to the feasibility of taking advantage of morpho-orthographic patterns language in order to assist with word processing in natural processing tasks.', 'I. INTRODUCTION\\n\\nResearch in neural network topologies and applications has demonstrated the ability of these computation devices to detect and efﬁciently use patterns in natural language processing tasks. More recently, these abilities have been advanced by making use of techniques such as convolutions in conjunction with massive amounts of data. For example, Krizhevsky, Sutskever, and Hinton obtained state-of-the-art results in clas- sifying images from the ImageNet LSVRC-2010 contest [3], and Mikolov et al developed a model that mapped one-hot- vector representations of words into a distributed vector space with interesting syntactic and semantic properties [4]. One characteristic of many of the algorithms used to generate such results is the fact that they need both a big training set and long training times [7].', 'In this paper we present results of training neural networks with techniques such as convolution, signal dimentionality reduction, cross-entropy, and others, in order to test com- putational algorithms proposed by other researchers in the linguistics literature as ways to map written letters into word semantic representations. In particular, we concentrate on the way in which input representations and details in topology', 'affect network performance, both from the point of view of ac- curacy and speed, since past research has found convolutional neural networks to be sensitive to such design decisions ([5], [7]). Our empirical results demonstrate considerable speed differences between networks using different topologies and input formats on the task of recreating the original word labels after passing them through a process that generates distributed vector representations. This result, specially in light of the smaller training set we use, could point to advantages of using input representations at the level of letters in natural language processing tasks, as they could facilitate taking advantage of morphological information. By doing so, neural networks could generalize into words they have not been pre- viously exposed to by developing word representations from the combination of word roots and additional morphemes. For example, with the ability to code the fact that verb + er generates a noun for someone that performs that verb action, a network could more easily learn a good semantic representation for hunter or runner after only having seen walker.', 'The rest of this paper is divided as follows: section II presents two models previously proposed by other researchers in the linguistics literature as mechanisms for mapping or- thographic symbols into semantic space, one which uses morphological information and one that does not; section III quickly discusses previous neural network architectures and other techniques used to map words into a distributed vector space; section IV presents the deep learning architecture used in our experiments; section V describes the conﬁguration parameters for our experiments; section VI presents empirical results we have obtained in our experiments; section VII discusses our results and presents conclusions; and section VIII suggest possible areas for future research based on the results we have obtained.\\n\\nII. ORTHOGRAPHY PROCESSING MODELS\\n\\nA. Dual-route Approach\\n\\nWithin the ﬁeld of linguistics, Jonathan Grainger and Jo- hannes Ziegler have proposed that human processing of words', 'involves mapping from orthographic elements into semantics via two separate routes [2]. One of these routes selects com- binations of letters that are the most informative in selecting word identity, called diagnosticity constraints. A second path in this model involves detecting sub-lexical phonological and morphological representations, called chunking constraints. These two paths lead to a combination of ﬁne-grained and coarse grained processing, which together allow for fast pro- cessing of orthographic symbols. A sketch of this model is shown in ﬁgure 1.', 'represented via a set of N input nodes. With this format, each word is presented to a neural network by activating one of these N nodes and leaving all other nodes inactive. In this manner, we can constructs a training set from online corpora by reading the text, dividing it into words, and automatically turning it into a sequence of one-hot vectors. This type of representation, therefore, allows us to leverage huge amounts of available data without having to tag each input element manually. At the same time, this type of representation keeps us from taking advantage of commonalities between different inputs that nevertheless share characteristics. For example, because the word runs and the word walks would be rep- resented by orthogonal vectors, a machine learning algorithm processing written text would have no knowledge that these are both verbs of movement/displacement in present tense singular form.', 'Fig. 1. The Dual-route Orthographic Processing Model proposed by Grainger and Ziegler\\n\\nto our research because it argues that an intermediate step in word processing is based on morpheme processing.\\n\\nThis topology is particularly important\\n\\nMuch work, therefore, has gone into how to turn one-hot vector representations into distributed vector representations. Mikolov et al [4] have presented a process through which one- hot-vector representations of words are turned into distributed vector representations by training a network to predict the context in which words appear in sentences, known as the skip- gram model. This approach is sketched in ﬁgure 2. Mikolov also discusses elsewhere in the literature how different al- gorithms for generating these vectors, or word embeddings, produce different results in syntactic and semantic tasks. In their experiments, these learning algorithms made use of between hundreds of thousands and billions of words in their training sets [7].', 'B. Two-layer Symbolic Network Model\\n\\nBaayen, Milin, Durdevic, Hendrix, and Merelli have ex- perimented with networks that map words into distributed representations coding the probability distribution of inﬂected forms without intervening morphological representations in highly agglutinating languages like Serbian [1]. These net- works, which connect their input layers directly to their output layers (i.e. do not make use of hidden layers) were found to be successful at predicting the kind of effects we see in humans, documented elsewhere in the morpheme processing literature. In their network model, word representations at the input layer were directly connected to the network’s output layer. Of particular importance to the experiments discussed in our paper is the fact this model, also known as the Naive Discriminatory Learning model, argues that word processing can be done without intermediate morphological representations.\\n\\nthat', 'that\\n\\nIII. WORD2VEC MAPPING OF WORDS FROM ONE-HOT-VECTORS TO DISTRIBUTED VECTOR REPRESENTATIONS\\n\\nIn a one-hot-vector representation of words, each word is represented via a set of nodes with cardinality equal to the number of words in the vocabulary. That is, N words are\\n\\nFig. 2. Skip-gram model. Provided with word w(t), learn to predict its context words.\\n\\nWhen training networks to correctly predict the probability of context words, Mikolov et al found that word projections into distributed vector space encoded linguistic patterns and regularities. For example, taking the vector for ”king”, sub- tracting the vector for ”man”, and then adding the vector for ”woman” produced a vector that was closest to the vector for ”queen” than to any other word.\\n\\nIV. FROM ONE-HOT-VECTOR REPRESENTATIONS OF LETTERS TO DISTRIBUTED SEMANTIC REPRESENTATIONS OF WORDS', 'In order to study the two different morpho-phonetic theories presented by Grainger and Ziegler and by Baayen et al, we have implemented neural network topologies that follow the concepts they describe, and tested them on online corpora. The experiments discussed in this paper implemented neural networks that incorporated concepts from these theories using several deep learning techniques. These networks received words as a sequence of one-hot-vector representation of letters and mapped them to word vector space through convolutions, noise contrastive estimation (NCE) [6] [4], dimentionality reduction, and cross-entropy learning. Having networks re- ceive letters as one-hot vectors also allowed us to incorporate mapping from orthography to representations at the level of words, something that both models mentioned above do. Like in the case of representing written text as a sequence of words in one-hot vector format, using one-hot vector representations of letters allows us to take advantage of online corpora, while also keeping us from taking advantage of commonalities be- tween letters. For example, no information would be provided informing an algorithm that e and a are both vowels. Mapping from one-hot vector representations of letters to a distributed representation of letters would allow us to take advantage of these commonalities, while at the same time allowing us to study morpho-orthographic linguistic theories of semantic word representations.', 'Following is a description of these networks in detail, before\\n\\npresenting results.\\n\\nA. From One-hot-vector Representation of Letters to a Dis- tributed Vector Space for Letters\\n\\nFig. 3. Letters are mapped from one-hot-vectors to distributed vectors by a network that learns to predict the context in which letters are used.', 'letters in one-hot-vector format. These network inputs had a cardinality of 784, equal to the length of the longest word (28) multiplied by the size of the letters one-hot vector represen- tations (28). These letters were then mapped to a continuous vector space by the letter embedding matrix developed above, thus generating the representation of a word as a stream of letters in continuous vector space. As the longest word in the vocabulary was 28 letters long, words at this point were represented by a stream of 28 X 5 = 140 ﬂoating point numbers. Words shorter than 28 letters long were zero-padded to the left and right. This part of the process is represented in ﬁgure 4 .', 'In our networks, letters were mapped to a vector space of cardinality 5. We chose this cardinality because we can represent 28 distinct letters with 5 nodes even if we used a purely binary representation. A purely binary representation could have led to letters being represented in ways where different letters did not share values for common attributes, but as we we will discuss later in this paper, this is not what the learning process converged towards. Learned letter representations both grouped similar types of letters together and were useful for the learning process in the rest of the network. This process generated a 28 X 5 letter embedding matrix to be used in other stages of processing, and is represented in ﬁgure 3.', 'Once this mapping to distributed space is completed, we use the resulting mapping matrix in a convolution that turns words, represented as a sequence of letters in one-hot-vector format, to words represented as the concatenation of distributed vec- tors for those same letters, as discussed further in the following sections.\\n\\nFig. 4. Words were mapped from a sequence of one-hot-vector letter representations to a sequence of distributed vector letter representation.\\n\\nB. Dual Route Convolutions\\n\\nAfter the training described above, we had a network that received a single word at a time, represented as a sequence of\\n\\nIn order to simulate Grainger and Ziegler’s dual route pro- cessing architecture, this representation was then fed through two parallel convolution layers. One of them, simulating the', 'chunking process of detecting consecutive letters, had the width of two letter representations (ten nodes, in our case), with a stride of one letter representation at a time, or ﬁve nodes. The second convolution layer, simulating the diagnos- ticity process of detecting letters in a word, was as wide as the longest word representation in the language, or 140 nodes. This convolution layer did not have a stride, as it covered the complete word at once. Both of these convolution layers had a depth of 784 (28 letters, squared), allowing the network to use each ﬁlter to detect a speciﬁc letter pair, if needed. This produced an output from the diagnosticity convolution of 21168 nodes (784 possible letter pairs, and a stride of one letter 27 times) and an output from the chunking convolution of 784 nodes (784 possible letter pairs, with no stride).\\n\\nC. Generation of Word Semantics and Word Labels', 'C. Generation of Word Semantics and Word Labels\\n\\nAll of these values where then passed to a fully connected layer with 50 nodes, reducing the data dimentionality. The value of 50 nodes was tested empirically, and produced better results than networks with 75, 100, 200, or 30 nodes at that particular layer. Values were then fed to a layer with 128 nodes, trained to output the distributed word representation obtained in the original word embedding process. For both of these distributed word representations, 128 nodes worked better than both 75 nodes and 256 nodes. Finally, in order to ensure that training moved parameters towards values that allowed recovering the original word, these 128 nodes were fed to an output layer with 50,000 nodes, which was trained via negative log likelihood to output the label for the word entered. The resulting complete network is shown in ﬁgure 5.', 'which they propose implements a Naive Discriminatory Learn- ing algorithm. Although they did not use their topology for the exact task we use here, they did train networks receiving information about co-occurrences of linguistic elements in corpora, which makes it an interesting topology to compare ours with, specially since their networks achieved results that mimicked human word processing without making use of ex- plicit morphological representations. Our version of a network that follows their approach starts by presenting words as one hot vector sequences of letters, convolving them with a letter embedding matrix constructed as before, and then directly feeding it to a layer that is trained to generate distributed word representations. This topology is shown in ﬁgure 6. As can be seen, the input and output signals to these two networks are the same, and the only difference is that the symbolic two layer network does not use dual route convolutions or dimension reduction layers.', 'Fig. 6. Naive Discriminatory Learning network\\n\\nIn order to test the effect of letter representations for these two topologies, we also ran experiments where the one hot vector representations of letters where not fed through the letter embedding matrix, instead feeding them directly to the rest of the network, with the appropriate changes in the dual route convolutions for those networks, and with no additional changes for the symbolic two layer networks. Finally, we tested the dual route networks with either the discriminality convolution by itself or the chunking convolution by itself in order to see what effect they each have on total network performance.\\n\\nV. EXPERIMENT PARAMETERS\\n\\nFig. 5. Complete Dual-Route NN architecture\\n\\nD. Comparison Networks', 'V. EXPERIMENT PARAMETERS\\n\\nFig. 5. Complete Dual-Route NN architecture\\n\\nD. Comparison Networks\\n\\nIn order to better evaluate the performance of the network described above, we ran the same task under other topologies. One of these topologies mimicked the symbolic two later net- work ﬁrst presented by Baayen, Milin, Hendrix, and Merelli,\\n\\nsentences trained All at previously from Wikipedia, http://www.mattmahoney.net/dc/textdata.html. exact vocabulary was formed by the 50,000 most common words in this set, replacing all other words with the token UNKNOWN. To generate word embeddings, the skip-gram model was conﬁgured to use two context words to the left and two context words to the right of the input word. We used the\\n\\nnetworks\\n\\nobtained\\n\\nwere\\n\\nwith\\n\\navailable The', 'same data set this time with to train letter embeddings, one context letter to the left and to the right. Both types of embeddings were trained using gradient descent with a learning rate of 1. Convolution, dimentionality reduction, and softmax layers in the implementations of the dual route and the two-layer layers used a learning rate of .01. Initially, letter word embeddings were trained for 200,000 epochs, embeddings were trained for 100,000 epochs, and dual route convolution layers were trained for 100,000 epochs. Each of these epochs used a different subset of data for training. initially training embedding matrices In all cases, after separately from complete networks, their values continued to be modiﬁed while they were part of other more complex networks. All networks were implemented in the Python programming language through the TensorFlow library [8]. All experiments were repeated 32 times, and the results presented in the next section discuss the average of these 32 runs.', 'VI. RESULTS\\n\\nA. Letter Embeddings', 'After networks were trained to predict the context in which letters were being used in the vocabulary, we looked at the embedding matrices that emerged, ﬁnding some patterns to the coding assigned to particular letters. Considering these embeddings as vectors in representation space, and computing the angle for vectors representing different pairs of letters, smaller angles indicated letters being close to each other in the vector space developed by the neural network learning process. This analysis revealed some expected commonalities based on the ways letters appear in words. For example, vector representations for vowels were most similar to that of other vowels, and ’i’ and ’y’ were always the closest letter to each other. Many other proximities were more obscure, specially among consonants, and will require further study to see what morpho-orthographic information they reveal. The consistent results seen on vowels, though, indicate the learning process can in fact capture letter use commonalities.', 'B. Effect of Topologies and Letter Representations on Learn- ing\\n\\nachieved much lower performance, regardless of number of epochs or learning rates used; those experiments were run for up to a million epochs, and with learning rates of .01, .1, .5, 1, and 5, never achieving an accuracy higher than .125.', 'While the ﬁnal accuracy values for these two topologies are very close to each other, the situation is different when we look at the negative log likelihood error values produced during the learning process. Because we trained networks to produce a single discrete label for each word, the error at the output layer used during gradient descent is given by e = − (cid:80)n 1 log(qi), where n is the size of the training set, and qi is the output for the node in the softmax output layer that corresponds to the correct label for each word in the training set. As seen in the error curves in ﬁgures 7 and 8, networks using letter embedding reach relatively stable values of training errors after about 20,000 epoch, while it takes much longer for networks that do not make use of distributed letter representations. After 20,000 epochs, dual route networks that used distributed letter representations had training error values of between 5.9 and 6.8, while networks that did not use distributed letter representations occasionally produced errors as high as 11.2. This difference in sensitivity to training data begins after networks continue to be trained once they have initially reached stable values, tending to indicate networks not using distributed letter representations are beginning to overﬁt for particular data characteristics in their training data. The two curves shown here display negative log error values as opposed to accuracy values, since it is the former that is used to drive gradient descent in these networks, and it is computationally expensive to determine the accuracy of a network after each training epoch.', 'TABLE I FINAL ACCURACY VALUES\\n\\nTopology Dual Route, Letter Embeddings Dual Route, One-hot Vectors Naive Discriminatory Learning, Letter Embeddings Naive Discriminatory Learning, One-hot Vectors Chunking Path Only Discriminatory Path Only\\n\\nAccuracy .4309 .4173 .4004 .1379 .4197 .3469', 'Accuracy .4309 .4173 .4004 .1379 .4197 .3469\\n\\n1) Dual-route topologies: Different network topologies performed differently depending on their using an interme- diate mapping to distributed letter representations or not. For networks with dual path topologies, the average accuracy when using letter embeddings and when not using them was of .4309 and .4173, respectively. While accuracy values of .4309 and .4173 might seem low, being below 5o% accuracy, we note that these networks are being required to assign a correct word label from among 50,000 possibilities. As a comparison, Mikolov et al report performance values of between 9% and 64% in a number of syntactic and semantic tasks, all while using between 24 million and 784 million words ([4]), as opposed to the 50,000 we use in our experiments. In addition, directly recreating word labels after only generating word embeddings, without processing words at the level of letters,', 'Fig. 7. Training of dual route networks using letter embeddings\\n\\n2) Symbolic Two layer (Naive Discriminatory Learning) topologies: Symbolic two layer (Naive Discriminatory Learn- ing) topologies perform slightly worse than dual route net-\\n\\nexact, particular example yet. In addition, networks provided with topological details that allow them to consider pairs of consecutive letters tend to perform better, suggesting that mor- phological information might be being captured and used. In addition, Naive Discriminatory Learning networks, which have no explicit mechanisms for processing/storing morphological information, take longer to achieve optimum accuracy, and are, even at that point, lower than those achieved by dual route topologies.\\n\\nFig. 8. Training of dual route networks using one-hot vector letter represen- tations\\n\\nVIII. FUTURE WORK', 'VIII. FUTURE WORK\\n\\nworks, with an average accuracy of .4004. These networks are much more sensitive to input format, whoever, having an accuracy of only .1379 when using one-hot vector letter representations. Another important difference between dual route and Naive Discriminatory Learning networks is the time that they take to achieve their ﬁnal accuracy values. As seen in ﬁgure 9, even under the best performing input representation, Naive Discriminatory Learning topologies take close to ten times as many epochs to achieve negative log likelihood values similar to those of dual route topologies.', 'The results presented here bring to mind several interesting avenues for future investigation. We plan to look at the nature of errors being made by networks of different topologies in order to better understand how they are obtaining their results, and what facilitates learning speed and resilience for the dual path letter embedding topologies. We are also interested in studying the letter proximities calculated from the letter embedding matrices, as it might be these proximities that help the learning process under a comparatively smaller training set. We also plan on controlling the number of letter combinations that are checked for in the letter convolution layers, as many letter combinations are bound to not be present in our vocabulary, and knowing which letter combinations are most important during the learning process could provide insights into language processing of both artiﬁcial neural network and humans. Finally, our future research will look into chunking paths that consider longer letter sequences, allowing for longer morphological units to be considered, such as -ing or -ment.', 'ACKNOWLEDGMENT\\n\\nFig. 9. Training of Naive Discriminatory Learning topology using letter embedding representations\\n\\nC. Effect of Separate Routes on Dual Route Topologies\\n\\nNeither route of the two route topology, taken separately, was able to achieve accuracy values as high as those obtained by the complete dual route topology, but chunking came close to it. Chunking paths achieved an average accuracy of .4197, while Diagnosticity paths achieved an average accuracy of .3469. This seems to indicate that most of the work of the network is being done by the chunking path, where letter pairs capable of forming morphological units are detected.\\n\\nVII. CONCLUSION', 'VII. CONCLUSION\\n\\nThe results outlined above indicate that topologies trained with letter embeddings reach optimum accuracy values faster, and are less sensitive to training data selection details. A possible explanation for those speed improvements and re- silience can be the fact that networks do not need to see as many individual examples of letters being used in particu- lar words, since embeddings provide distributed information. This information can be used when processing different but similar letter combinations that might not have been in an\\n\\nThe authors would like to thank: Josiah Erickson, Associate Director of IT for Infrastructure and Communication Systems, for his work on installing, conﬁguring, and maintaining the computer system where experiments were run; and Matthew Fullmer, who completed his undergraduate degree at Hamp- shire College in May 2017, for his work with Python code that was later incorporated into the one we used in our experiments.\\n\\nREFERENCES', 'REFERENCES\\n\\n[1] Baayen, R. Harald and Milin, Peter and Filipovic Durdevic, Dusica, and Hendrix, Peter, and Marelli, Marco. An amorphous model for morpho- logical processing in visual comprehension based on naive discriminative learning, Psychological Review, 2011.\\n\\n[2] Grainger, Jonathan and Ziegler, Johannes, A Dual-Route Approach to Orthographic Processing, Frontiers in Psychology, Volume 2, 2011 [3] Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey. ImageNet Clas- siﬁcation with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems 25, 2012\\n\\n[4] Mikolov, Thomas, Sutskever, Ilya, Chen, Kai, Corrado, Greg, and Dean, Jeffrey. Distributed Representations of Words and Phrases and their Compositionality, Advances in neural Information processing Systems 26 (NIPS 2013).', '[5] Zhang, Ye, Wallace, Byron. A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classiﬁcation. Proceedings of the 8th International Joint Conference on Natural Lan- guage Processing. 2017.\\n\\n[6] Mnih, Andriy, Kavukcuoglu, Koray. Learning word embeddings efﬁciently with noise-contrastive estimation. Neural Information processing Systems (NIPS) 2013.\\n\\n[7] Mikolov, Thomas, Chen, Kai, Corrado, Greg, Dean, Jeffrey. Efﬁcient Esti- mation of Word Representations in Vector Space International Conference on Learning Representation. 2013', '[8] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irv- ing, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Man- junath Kudlur, Josh Levenberg, Dan Man, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.']\n"
     ]
    }
   ],
   "source": [
    "print(research_paper1_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd204b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain.vectorstores.faiss.FAISS object at 0x7f7b7baa6fa0>\n"
     ]
    }
   ],
   "source": [
    "print(doc_search_paper1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69d24e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
